{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_score, recall_score, f1_score, precision_recall_curve,auc\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4caa46f",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "\n",
    "- The merged dataset containers data leakage features which are features that wouldnt be available in real-world prediction such as `max_delay`, `avg_delay`, `num_bad_months`, `months_total`. The target variable `label`, `id` and `amt_income_total` (to use the log transformed income) are also dropped. The target variable `label` is converted to integer for classification\n",
    "- Applied one-hot encoding to categorical variables \n",
    "- Applied train-test split and SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae02ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean_merged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].astype('int')\n",
    "leaky_features = ['max_delay', 'avg_delay', 'num_bad_months', 'months_total']\n",
    "X = df.drop(columns=['label', 'id', 'amt_income_total'] + leaky_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "if len(categorical_cols) > 0:\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True) #one-hot encoding\n",
    "else:\n",
    "    X_encoded = X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7beaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# apply SMOTE \n",
    "smote = SMOTE(random_state=42)\n",
    "X_balance, y_balance = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Class distribution of y after SMOTE: {Counter(y_balance)}\")\n",
    "print(f\"Class distribution of y without SMOTE: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9148db7",
   "metadata": {},
   "source": [
    "## 2. Random Forest Model (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45dcbdd",
   "metadata": {},
   "source": [
    "### 2.1 Baseline model\n",
    "\n",
    "Random Forest is an ensemble learning method based on building multiple decision trees during training and combining their predictions by majority vote for classification. \n",
    "\n",
    "This makes the model:\n",
    "- Generalise better and avoid overfitting\n",
    "- Handle non-linear relationships\n",
    "- Automatically estimate feature importance\n",
    "\n",
    "The first model is a baseline model that is simple and unoptimised version that uses the original imbalanced data with default Random Forest hyperparameters to provide a reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea00353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))  # y_test loaded as true test targets\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767534e2",
   "metadata": {},
   "source": [
    "### 2.2 Baseline Model (using only the top 10 most important features)\n",
    "\n",
    "Rationale for reducing to top 10 most important features:\n",
    "- Interpretability \n",
    "- Eliminates noisy or irrelevant features that might affect accuracy of model\n",
    "- Reduce likelihood of model learning random patterns \n",
    "- Fewer inputs = Increase training efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ec2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances and feature names\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Make a DataFrame for easy sorting/viewing\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "feat_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top N features (e.g. top 15)\n",
    "N = 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_imp_df['feature'][:N][::-1], feat_imp_df['importance'][:N][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top Feature Importances from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feat_imp_df.head(10))  # Show top 10 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3244133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce train and test to selected features\n",
    "\n",
    "N = 10\n",
    "top_features = feat_imp_df['feature'].iloc[:N].tolist()\n",
    "top_features\n",
    "\n",
    "X_train_reduced = X_train[top_features]\n",
    "X_test_reduced = X_test[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reduced = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_pred_reduced = rf_reduced.predict(X_test_reduced)\n",
    "\n",
    "print(\"Accuracy with reduced features:\", accuracy_score(y_test, y_pred_reduced))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_reduced))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64787df",
   "metadata": {},
   "source": [
    "After feature reduction:\n",
    "- Accuracy improved slightly: 0.9809380142622052 to 0.9810751508502469 \n",
    "- Recall for target label 0 (no approval) is still 1.00\n",
    "- Recall for target label 1 (approval) improved slightly: 0.15 to 0.16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_reduced = rf_reduced.predict_proba(X_test_reduced)[:, 1]  # Probability of class '1'\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_reduced)  # y_test = true labels\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_proba_reduced)\n",
    "print(f\"AUC: {auc_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC Curve\n",
    "plt.figure(figsize = (6, 5))\n",
    "plt.plot(fpr, tpr, label = f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label = 'Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec70765",
   "metadata": {},
   "source": [
    "### 2.3 GridSearchCV to find the best combination of hyperparameters for Random Forest Model \n",
    "\n",
    "Rationale for GridSearchCV:\n",
    "- We want to improve model performance (measured by ROC-AUC) by testing different parameter settings (n_estimators, max_depth, min_samples_split) systematically.\n",
    "\n",
    "Key Random Forest hyperparameters:\n",
    "- `n_estimators`: Number of trees in the Random Forest. Having more trees can improve model performance but increase training time. \n",
    "- `max_depth`: Maximum depth of each tree which controls the model complexity\n",
    "- `min_samples_split`: Minimum samples required to split a node. Larger `min_samples_split` prevents overfitting as it requires more samples to create a split.\n",
    "\n",
    "#### How the GridSearchCV works?\n",
    "\n",
    "GridSearchCV tries every combination in the grid, using cross-validation (CV) to evaluate each combination. We chose cv = 3 (3-fold CV splits), splits the data into 3 parts. The model is trained 3 rounds, each round, the model is trained on a different 2/3 of the data. This CV ensures the model generalises well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state = 42),\n",
    "    param_grid,\n",
    "    cv = 3,\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs = 1,       # <<< Only use 1 job (no parallel, less stress on env)\n",
    "    verbose = 2\n",
    ")\n",
    "grid_rf.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ccdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Random Forest params:\", grid_rf.best_params_)\n",
    "print(\"Best cross-validated AUC:\", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb7a7e",
   "metadata": {},
   "source": [
    "The performance of the optimal hyperparameter combination identified by GridSearchCV was evaluated using the ROC-AUC metric.\n",
    "\n",
    "ROC-AUC measures how well the Random Forest model separates the 2 classes (defaulters vs. non-defaulters)\n",
    "\n",
    "Rationale for using ROC-AUC:\n",
    "- We have identified the dataset to be imbalanced, so using accuracy metric can be misleading. A model that always predicts the majority class could still appear \"accurate\", even though it performs poorly on the minority class. Thus, the model might fail to identify important minority cases, like defaulters, which could cause harm to the bank.\n",
    "- In contrast, ROC-AUC looks at how well the model separates defaulters vs. non-defaulters, so it is much more reliable compared to accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b012b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Best Model on Test Set\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict probabilities (for AUC) and classes\n",
    "y_proba = grid_rf.best_estimator_.predict_proba(X_test_reduced)[:, 1]\n",
    "y_pred = grid_rf.best_estimator_.predict(X_test_reduced)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Test set ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213f71a",
   "metadata": {},
   "source": [
    "Based on the Classification Report, even though the ROC-AUC shows a high value of 0.80, the recall value is low, and false negative rate is very high (0.88). In the financial institution context, type II error which is the false negative rate might be a greater consequence. The model would approve a credit card for someone who is actually a bad customer (likely to default) the bank loses money if the applicant with a poor credit history gets approved, and later fails to pay. Hence, to improve on this, section 3 contains different enhancements made to the baseline model to improve the performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b946a1",
   "metadata": {},
   "source": [
    "## 3. Random Forest Model Enhancements / Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46f467",
   "metadata": {},
   "source": [
    "### 3.1 Random Forest Model with SMOTE and Fixed Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialise Random Forest \n",
    "rf_model_smote = RandomForestClassifier(\n",
    "    n_estimators = 100,        # no. of trees\n",
    "    max_depth = 10,           \n",
    "    min_samples_split = 20,   \n",
    "    min_samples_leaf = 10,\n",
    "    max_features = 'sqrt',     # random feature selection, diff trees see different subset of features \n",
    "    bootstrap = True,          # bagging, ensemble learning\n",
    "    random_state = 42,\n",
    "    n_jobs = -1               # for parallel processing\n",
    ")\n",
    "\n",
    "rf_model_smote.fit(X_balance, y_balance)\n",
    "\n",
    "#  predictions\n",
    "y_pred_smote = rf_model_smote.predict(X_test)\n",
    "y_pred_proba_smote = rf_model_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# performance metrics \n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred_proba_smote)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_smote):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_smote):.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "\n",
    "# confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_smote)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nRates:\")\n",
    "total_0 = cm[0,0] + cm[0,1]  \n",
    "total_1 = cm[1,0] + cm[1,1] \n",
    "\n",
    "\n",
    "print(f\"\\nSpecificity (True Negative Rate): {cm[0,0]/total_0:.4f}\")\n",
    "print(f\"False Positive Rate: {cm[0,1]/total_0:.4f}\")\n",
    "print(f\"Sensitivity (Recall/True Positive Rate): {cm[1,1]/total_1:.4f}\")\n",
    "print(f\"False Negative Rate: {cm[1,0]/total_1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve \n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_smote)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred_proba_smote):.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f22b0c",
   "metadata": {},
   "source": [
    "Issues with random forest model using training set balanced using smote:\n",
    "\n",
    "1. AUC of 0.6 is only slightly better than random guesssing, contradicts the 0.90 accuracy rate. The accuracy rate might be misleading because majority class dominates (`label` = 0 for non default). The model might not be discriminating well between the 2 classes. \n",
    "\n",
    "2. Although recall increased from the baseline model (0.20 from 0.11), the trade off is lower precision and f1 score as these are nearly 0 for `label` = 1. Since the minority class is extremely small (about 0.02%), SMOTE might produce unrealisitc synthetic minority points, and the model might be overfitting to the SMOTE-generated data by learning the artificial patterns created by SMOTE but not generalizing well to the test set. Some features are dominated by 0s or categorical dummies.\n",
    "\n",
    "3. Potential changes made to this model: \n",
    " - to lower the threshold to 0.3 from 0.5 to capture more defaulters (true positives) (refer to section 3.2)\n",
    " - to change to class_weight=\"balanced\" to replace SMOTE (refer to section 3.4)\n",
    " - change the model with features that are engineered (refer to section 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b2ac4",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest Model (adjusted threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_thres = RandomForestClassifier(\n",
    "    n_estimators=100,        # no. of trees\n",
    "    max_depth=10,           \n",
    "    min_samples_split=20,   \n",
    "    min_samples_leaf=10,\n",
    "    max_features='sqrt',     # random feature selection, diff trees see different subset of features \n",
    "    bootstrap=True,          # bagging, ensemble learning\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # for parallel processing\n",
    ")\n",
    "\n",
    "rf_model_thres.fit(X_balance, y_balance)\n",
    "\n",
    "y_pred_thres = rf_model_thres.predict(X_test)\n",
    "y_pred_proba_thres = rf_model_thres.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.3\n",
    "y_pred_adj = (y_pred_proba_thres >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# performance metrics \n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred_proba_thres)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_adj):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_thres):.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_adj))\n",
    "\n",
    "# confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_adj)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nRates:\")\n",
    "total_0 = cm[0,0] + cm[0,1]  \n",
    "total_1 = cm[1,0] + cm[1,1] \n",
    "\n",
    "print(f\"\\nSpecificity (True Negative Rate): {cm[0,0]/total_0:.4f}\")\n",
    "print(f\"False Positive Rate: {cm[0,1]/total_0:.4f}\")\n",
    "print(f\"Sensitivity (Recall/True Positive Rate): {cm[1,1]/total_1:.4f}\")\n",
    "print(f\"False Negative Rate: {cm[1,0]/total_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c76746",
   "metadata": {},
   "source": [
    "Recall for `label` = 1 increase significantly (0.6) but at the cost of much lower precision, F1-score and accuracy. Most non-defaulters are now wrongly flagged as defaulters. Adjusting threshold does not quite solve the issue since the model cannot tell defaulters apart confidently. Even at threshold 0.3, only a small group of samples truly correspond to real defaulters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625fead",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest Model with Class-Weight Balancing Instead of SMOTE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a4187",
   "metadata": {},
   "source": [
    "To address the severe class imbalance in the dataset, we trained a Random Forest with class_weight='balanced'. This approach automatically assigns higher importance to the minority class (defaulters) during training, without generating synthetic samples as in SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model_cw = RandomForestClassifier(\n",
    "    n_estimators = 200,\n",
    "    max_depth = 15,\n",
    "    min_samples_leaf = 5,\n",
    "    min_samples_split = 10,\n",
    "    max_features = 'sqrt',\n",
    "    class_weight = 'balanced',\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "rf_model_cw.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_cw = rf_model_cw.predict(X_test)\n",
    "y_pred_proba_cw = rf_model_cw.predict_proba(X_test)[:, 1] \n",
    "\n",
    "cv_scores_rf = cross_val_score(rf_model_cw, X_train, y_train, cv = 5, scoring = 'roc_auc')\n",
    "print(f\"Cross-Validation AUC: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_pred_proba_cw)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_cw):.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_cw):.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "print(\"\\n Classification Report\")\n",
    "print(classification_report(y_test, y_pred_cw))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_cw)\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nRates:\")\n",
    "\n",
    "total_0 = cm[0,0] + cm[0,1]  \n",
    "total_1 = cm[1,0] + cm[1,1] \n",
    "\n",
    "print(f\"\\nSpecificity (True Negative Rate): {cm[0,0]/total_0:.4f}\")\n",
    "print(f\"False Positive Rate: {cm[0,1]/total_0:.4f}\")\n",
    "print(f\"Sensitivity (Recall/True Positive Rate): {cm[1,1]/total_1:.4f}\")\n",
    "print(f\"False Negative Rate: {cm[1,0]/total_1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision recall curve\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(recall, precision, color='b', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precisionâ€“Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9d352",
   "metadata": {},
   "source": [
    "Most of the performance metric values improved quite a bit from the model that uses smote especially recall, false negative rate, pr auc which are most important as having high false negative rate can be quite costly if a defaulter is wrongly predicted as non-defaulter. Even though PR AUC improved slightly, but the model may be doing slightly better than random guessing only because the features might not provide enough information to distinguish between defaulters and non-defaulters. The model still struggles to detect the minority class effectively. Feature engineering will be done under the section 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340805a1",
   "metadata": {},
   "source": [
    "### 3.4 Random Forest Model  With Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db85865",
   "metadata": {},
   "source": [
    "####   3.4.1 Check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_encoded.columns,\n",
    "    'importance': rf_model_cw.feature_importances_\n",
    "}).sort_values('importance', ascending = False)\n",
    "\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10323cbe",
   "metadata": {},
   "source": [
    "Based on the feature importance for the top 10 features, only the top 3 features seem to have predictive power as all other features have importance of less than 0.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d464ebd",
   "metadata": {},
   "source": [
    "#### 3.4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def new_features(X):\n",
    "   \n",
    "    X_eng = X.copy()\n",
    "    \n",
    "    # use ratio\n",
    "    if 'years_employed' in X.columns and 'age' in X.columns:\n",
    "        X_eng['career_ratio'] = X['years_employed'] / (X['age'] + 1)\n",
    "        X_eng['employment_stability'] = X['years_employed'] / (X['age'] - 18 + 1)  # Years since adult\n",
    "    \n",
    "    if 'amt_income_log' in X.columns and 'cnt_fam_members' in X.columns:\n",
    "        X_eng['income_per_member'] = X['amt_income_log'] / (X['cnt_fam_members'] + 1)\n",
    "        X_eng['log_income_per_member'] = np.log1p(X_eng['income_per_member'])\n",
    "    \n",
    "    # interaction terms\n",
    "    if 'age' in X.columns and 'amt_income_log' in X.columns:\n",
    "        X_eng['age_income_interaction'] = X['age'] * X['amt_income_log']\n",
    "        X_eng['age_income_ratio'] = X['age'] / (X['amt_income_log'] + 1)\n",
    "    \n",
    "    # nonlinear transf\n",
    "    if 'age' in X.columns:\n",
    "        X_eng['age_squared'] = X['age'] ** 2\n",
    "        X_eng['age_cubed'] = X['age'] ** 3\n",
    "        X_eng['log_age'] = np.log1p(X['age'])\n",
    "    \n",
    "    if 'years_employed' in X.columns:\n",
    "        X_eng['employment_squared'] = X['years_employed'] ** 2\n",
    "        X_eng['log_employment'] = np.log1p(X['years_employed'] + 1)\n",
    "    \n",
    "\n",
    "    return X_eng\n",
    "\n",
    "# apply the feature engineering defined above\n",
    "X_engineered = new_features(X_encoded)\n",
    "\n",
    "# use one-hot encoding for cat variables\n",
    "categorical_eng_cols = X_engineered.select_dtypes(include=['object', 'category']).columns\n",
    "if len(categorical_eng_cols) > 0:\n",
    "    X_engineered = pd.get_dummies(X_engineered, columns=categorical_eng_cols, drop_first=True)\n",
    "\n",
    "X_train_eng, X_test_eng, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#  feature selection using correlation with target\n",
    "correlations = X_train_eng.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "\n",
    "# select top correlated features\n",
    "selected_features = correlations[correlations > 0.01].index.tolist()\n",
    "if len(selected_features) < 20: \n",
    "    selected_features = correlations.head(30).index.tolist()\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features for modeling\")\n",
    "\n",
    "X_train_selected = X_train_eng[selected_features]\n",
    "X_test_selected = X_test_eng[selected_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cedeb9",
   "metadata": {},
   "source": [
    "#### 3.4.3 Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_engineered = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_engineered.fit(X_train_selected, y_train)\n",
    "\n",
    "\n",
    "cv_scores = cross_val_score(rf_engineered, X_train_selected, y_train, \n",
    "                           cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"Cross-Validation AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "y_pred_eng = rf_engineered.predict(X_test_selected)\n",
    "y_pred_proba_eng = rf_engineered.predict_proba(X_test_selected)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aceb3dd",
   "metadata": {},
   "source": [
    "#### 3.4.4 Evaluation of Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c52022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model_name, y_test, y_pred_eng, y_pred_proba_eng, feature_names, model):\n",
    "    print(f\"\\n{model_name} Evaluation\")\n",
    "\n",
    "\n",
    "    roc_auc_value = roc_auc_score(y_test, y_pred_proba_eng)\n",
    "    acc = accuracy_score(y_test, y_pred_eng)\n",
    "    prec = precision_score(y_test, y_pred_eng)\n",
    "    rec = recall_score(y_test, y_pred_eng)\n",
    "    f1 = f1_score(y_test, y_pred_eng)\n",
    "    pr_precision, pr_recall, _ = precision_recall_curve(y_test, y_pred_proba_eng)\n",
    "    pr_auc_value = auc(pr_recall, pr_precision)\n",
    "\n",
    "\n",
    "    print(f\"ROC AUC: {roc_auc_value:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"PR AUC:  {pr_auc_value:.4f}\") \n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\nConfusion Matrix\")\n",
    "    cm = confusion_matrix(y_test, y_pred_eng)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\n Classification Report\")\n",
    "    print(classification_report(y_test, y_pred_eng))\n",
    "   \n",
    "    total_0 = cm[0,0] + cm[0,1]\n",
    "    total_1 = cm[1,0] + cm[1,1]\n",
    "\n",
    "    print(f\"\\nRates:\")\n",
    "\n",
    "    print(f\"\\nSpecificity (True Negative Rate): {cm[0,0]/total_0:.4f}\")\n",
    "    print(f\"False Positive Rate: {cm[0,1]/total_0:.4f}\")\n",
    "    print(f\"Sensitivity (Recall/True Positive Rate): {cm[1,1]/total_1:.4f}\")\n",
    "    print(f\"False Negative Rate: {cm[1,0]/total_1:.4f}\")\n",
    "\n",
    "    # for plotting in part 2.4.5\n",
    "    return {\n",
    "        'roc_auc': roc_auc_value,\n",
    "        'pr_auc': pr_auc_value,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "#1.  evaluate engineered model\n",
    "results_eng = evaluation(\n",
    "    \"Random Forest with Feature Engineering\", \n",
    "    y_test, y_pred_eng, y_pred_proba_eng, \n",
    "    selected_features, rf_engineered\n",
    ")\n",
    "\n",
    "\n",
    "# 2.  train \"baseline\" model (using class weight (rf_model_cw), no feature engineering)\n",
    "\n",
    "results_cw = evaluation(\n",
    "    \"Random Forest (No Engineering)\", \n",
    "    y_test, y_pred_cw, y_pred_proba_cw, \n",
    "    X_train.columns, rf_model_cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c82e7",
   "metadata": {},
   "source": [
    "For the random forest model with feature engineered features, most of the performance metrics improved quite a bit as compared to the baseline model of random forest using class-weighting. \n",
    "1. The model is moderately strong. It performs significantly better than random guessing and slightly better than the baseline\n",
    "2. After feature engineering, the model identifies more true positives with slightly better confidence, however, more needs to be done to reduce the false negative rate which can be very costly error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c73a2",
   "metadata": {},
   "source": [
    "#### 3.4.5 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# auc roc curve comparison plot\n",
    "plt.subplot(2, 3, 1)\n",
    "fpr_eng, tpr_eng, _ = roc_curve(y_test, y_pred_proba_eng)\n",
    "fpr_base, tpr_base, _ = roc_curve(y_test, y_pred_proba_cw)\n",
    "\n",
    "plt.plot(fpr_eng, tpr_eng, label=f'With Feature Engineering (AUC = {results_eng[\"roc_auc\"]:.4f})', linewidth=2)\n",
    "plt.plot(fpr_base, tpr_base, label=f'Baseline (AUC = {results_cw[\"roc_auc\"]:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "#  feature importance plot (show new features)\n",
    "plt.subplot(2, 3, 2)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': rf_engineered.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Top 15 Feature Importances\\n(Engineered Model)')\n",
    "plt.xlabel('Importance Score')\n",
    "\n",
    "# performance metrics comparison plot\n",
    "plt.subplot(2, 3, 3)\n",
    "metrics = ['ROC AUC', 'PR AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "base_values = [results_cw['roc_auc'], results_cw['pr_auc'],results_cw['accuracy'], results_cw['precision'], \n",
    "               results_cw['recall'], results_cw['f1']]\n",
    "eng_values = [results_eng['roc_auc'], results_cw['pr_auc'], results_eng['accuracy'], results_eng['precision'], \n",
    "              results_eng['recall'], results_eng['f1']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, base_values, width, label='Baseline', alpha=0.7)\n",
    "plt.bar(x + width/2, eng_values, width, label='With Engineering', alpha=0.7)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Comparison')\n",
    "plt.xticks(x, metrics, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# precision recall curve plot\n",
    "plt.subplot(2, 3, 4)\n",
    "precision_eng, recall_eng, _ = precision_recall_curve(y_test, y_pred_proba_eng)\n",
    "precision_base, recall_base, _ = precision_recall_curve(y_test, y_pred_proba_cw)\n",
    "\n",
    "plt.plot(recall_eng, precision_eng, label='With Feature Engineering', linewidth=2)\n",
    "plt.plot(recall_base, precision_base, label='Baseline', linewidth=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ded3d",
   "metadata": {},
   "source": [
    "## 3. XGBOOST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c9d6a",
   "metadata": {},
   "source": [
    "#### 3.1 XGBoost Model (version 1)\n",
    "\n",
    "Rationale of XGBoost with `scale_pos_weight1` & regularisation:\n",
    "- Class imbalance handling using `scale_pos_weight = (# negatives / # positives)` which penalises mistakes on minority class more (defaulters). This would improve recall on the minority (defaulters).\n",
    "- Regularisation using `reg_alpha = 0.1` and `reg_lambda = 0.1` to reduce overfitting \n",
    "\n",
    "We chose log loss as the evaluation metric, because it trains XGBoost to penalise the model for being confidently wrong (in this case wrongly predicting defaulters as non-defaulters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b8c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## XGBoost (with Scale Pos Weight) and with regularisation\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,   \n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"Cross-Validation AUC: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std() * 2:.4f})\")\n",
    "\n",
    "## COMPREHENSIVE EVALUATION FUNCTION\n",
    "def evaluate_model(model_name, y_true, y_pred, y_pred_proba):\n",
    "    print(f\"\\n--- {model_name.upper()} EVALUATION ---\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"TN: {cm[0,0]}, FP: {cm[0,1]}, FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
    "    \n",
    "    # Rates\n",
    "    total_0 = cm[0,0] + cm[0,1]\n",
    "    total_1 = cm[1,0] + cm[1,1]\n",
    "    print(f\"Specificity: {cm[0,0]/total_0:.4f}\")\n",
    "    print(f\"False Positive Rate: {cm[0,1]/total_0:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {cm[1,1]/total_1:.4f}\")\n",
    "    print(f\"False Negative Rate: {cm[1,0]/total_1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "results = []\n",
    "results.append(evaluate_model(\"Random Forest With Class Weight\", y_test, y_pred_cw, y_pred_proba_cw))\n",
    "results.append(evaluate_model(\"XGBoost\", y_test, y_pred_xgb, y_pred_proba_xgb))\n",
    "\n",
    "## COMPARISON SUMMARY\n",
    "\n",
    "print(\"\\nMODEL COMPARISON SUMMARY\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(comparison_df[['model', 'auc', 'accuracy', 'precision', 'recall', 'f1']].round(4))\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_cw)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest with Class Weight (AUC = {roc_auc_score(y_test, y_pred_proba_cw):.4f})')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_score(y_test, y_pred_proba_xgb):.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "plt.subplot(1, 3, 2)\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_pred_proba_cw)\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "\n",
    "plt.plot(recall_rf, precision_rf, label='Random Forest with Class Weight')\n",
    "plt.plot(recall_xgb, precision_xgb, label='XGBoost')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "#  Metrics Comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "metrics = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "rf_metrics = [results[0]['auc'], results[0]['accuracy'], results[0]['precision'], \n",
    "              results[0]['recall'], results[0]['f1']]\n",
    "xgb_metrics = [results[1]['auc'], results[1]['accuracy'], results[1]['precision'], \n",
    "               results[1]['recall'], results[1]['f1']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, rf_metrics, width, label='Random Forest with Class Weight', alpha=0.7)\n",
    "plt.bar(x + width/2, xgb_metrics, width, label='XGBoost', alpha=0.7)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metrics Comparison')\n",
    "plt.xticks(x, metrics, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb6f11",
   "metadata": {},
   "source": [
    "#### 3.2 XGBoost Model (with GridSearchCV to get optimal hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688222e",
   "metadata": {},
   "source": [
    "Then, we used GridSearchCV to tune the model's parameters and identify the best settings using ROC-AUC as the optimisation metric. Finally, we compared both models on the test set using various evaluation metrics: accuracy, ROC-AUC, precision, recall, and confusion matrix.\n",
    "\n",
    "We chose log loss as the evaluation metric, because it trains XGBoost to penalise the model for being confidently wrong (in this case wrongly predicting defaulters as non-defaulters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. BASELINE RANDOM FOREST (no feature engineering and no tuning)\n",
    "# ================================\n",
    "rf_model = RandomForestClassifier(random_state=42)  # <- strictly default\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_pred      = rf_model.predict(X_test)\n",
    "rf_proba     = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Baseline Random Forest ===\")\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_pred_proba_cw))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_cw))\n",
    "print(confusion_matrix(y_test, y_pred_cw))\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2. XGBOOST (with feature engineering) from above \n",
    "# ================================\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 3. TUNED XGBOOST (GridSearchCV)\n",
    "# ================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Assumptions (already defined above):\n",
    "# X_train_selected, X_test_selected  -> FE/selected columns for XGB\n",
    "# y_train, y_test\n",
    "\n",
    "# Compute class imbalance weight ON TRAIN ONLY\n",
    "pos_weight = (y_train == 0).sum() / max(1, (y_train == 1).sum())\n",
    "\n",
    "\n",
    "# Base estimator (same assumptions as above)\n",
    "xgb_base_for_grid = XGBClassifier(\n",
    "    n_estimators=200,          # will be overridden by grid\n",
    "    max_depth=15,             # will be overridden by grid\n",
    "    learning_rate=0.1,        # may be overridden by grid\n",
    "    scale_pos_weight=pos_weight,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [7, 15],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1],\n",
    "    'reg_lambda': [0.0, 0.1]\n",
    "}\n",
    "\n",
    "# For imbalanced classification, StratifiedKFold ensures that each fold has a similar positive/negative ratio.\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=xgb_base_for_grid,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run grid search on your feature engineered data\n",
    "grid_xgb.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best estimator and make predictions on the test set\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "print(\"Best Parameters:\", grid_xgb.best_params_)\n",
    "print(f\"Best Mean CV ROC AUC: {grid_xgb.best_score_:.4f}\")\n",
    "\n",
    "# Predict on TEST (same selected FE features)\n",
    "y_pred_xgb_tuned   = best_xgb.predict(X_test_selected)\n",
    "y_proba_xgb_tuned  = best_xgb.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "\n",
    "# Evaluate on TEST\n",
    "test_roc = roc_auc_score(y_test, y_proba_xgb_tuned)\n",
    "test_ap  = average_precision_score(y_test, y_proba_xgb_tuned)  # PR-AUC (AP)\n",
    "print(\"\\n=== Tuned XGBoost (GridSearchCV) ===\")\n",
    "print(f\"Test ROC AUC: {test_roc:.4f}\")\n",
    "print(f\"Test PR  AUC: {test_ap:.4f}\")\n",
    "print(f\"Test Accuracy:  {accuracy_score(y_test, y_pred_xgb_tuned):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_pred_xgb_tuned, zero_division=0):.4f}\")\n",
    "print(f\"Test Recall:    {recall_score(y_test, y_pred_xgb_tuned, zero_division=0):.4f}\")\n",
    "print(f\"Test F1-Score:  {f1_score(y_test, y_pred_xgb_tuned, zero_division=0):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb_tuned, digits=4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------\n",
    "# 4. PLOT ROC CURVES FOR ALL MODELS\n",
    "# ---------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf  = rf_pred\n",
    "y_proba_rf = rf_proba\n",
    "\n",
    "# Baseline XGBoost (feature-engineered baseline in your flow)\n",
    "y_pred_xgb_fe  = y_pred_xgb\n",
    "y_proba_xgb_fe = y_pred_proba_xgb\n",
    "\n",
    "# Tuned XGBoost (feature-engineered + GridSearchCV)\n",
    "y_pred_xgb_tuned  = y_pred_xgb_tuned       \n",
    "y_proba_xgb_tuned = y_proba_xgb_tuned       \n",
    "# --- ROC Curves ---\n",
    "fpr_rf,    tpr_rf,    _ = roc_curve(y_test, rf_proba)\n",
    "fpr_fe,    tpr_fe,    _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "fpr_tuned, tpr_tuned, _ = roc_curve(y_test, y_proba_xgb_tuned)\n",
    "\n",
    "auc_rf    = roc_auc_score(y_test, rf_proba)\n",
    "auc_fe    = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "auc_tuned = roc_auc_score(y_test, y_proba_xgb_tuned)\n",
    "\n",
    "# --- Precision-Recall Curves ---\n",
    "prec_rf,    recall_rf,    _ = precision_recall_curve(y_test, rf_proba)\n",
    "prec_fe,    recall_fe,    _ = precision_recall_curve(y_test, y_pred_proba_xgb)\n",
    "prec_tuned, recall_tuned, _ = precision_recall_curve(y_test, y_proba_xgb_tuned)\n",
    "\n",
    "ap_rf    = average_precision_score(y_test, rf_proba)\n",
    "ap_fe    = average_precision_score(y_test, y_pred_proba_xgb)\n",
    "ap_tuned = average_precision_score(y_test, y_proba_xgb_tuned)\n",
    "\n",
    "# --- Metrics for bar plot ---\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "\n",
    "rf_scores = [\n",
    "    accuracy_score(y_test, rf_pred),\n",
    "    precision_score(y_test, rf_pred, zero_division=0),\n",
    "    recall_score(y_test, rf_pred, zero_division=0),\n",
    "    f1_score(y_test, rf_pred, zero_division=0)\n",
    "]\n",
    "fe_scores = [\n",
    "    accuracy_score(y_test, y_pred_xgb),\n",
    "    precision_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    recall_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    f1_score(y_test, y_pred_xgb, zero_division=0)\n",
    "]\n",
    "tuned_scores = [\n",
    "    accuracy_score(y_test, y_pred_xgb_tuned),\n",
    "    precision_score(y_test, y_pred_xgb_tuned, zero_division=0),\n",
    "    recall_score(y_test, y_pred_xgb_tuned, zero_division=0),\n",
    "    f1_score(y_test, y_pred_xgb_tuned, zero_division=0)\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": metrics,\n",
    "    \"RF (no FE)\": rf_scores,\n",
    "    \"XGB (FE)\": fe_scores,\n",
    "    \"XGB Tuned (FE)\": tuned_scores\n",
    "})\n",
    "\n",
    "# --- Create subplots ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# (1) ROC curve\n",
    "axes[0].plot(fpr_rf,    tpr_rf,    label=f\"RF (AUC = {auc_rf:.3f})\")\n",
    "axes[0].plot(fpr_fe,    tpr_fe,    label=f\"XGB (FE) (AUC = {auc_fe:.3f})\")\n",
    "axes[0].plot(fpr_tuned, tpr_tuned, label=f\"XGB Tuned (FE) (AUC = {auc_tuned:.3f})\")\n",
    "axes[0].plot([0, 1], [0, 1], linestyle=\"--\", color='gray')  # chance line\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].set_title(\"ROC Curve\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# (2) Precisionâ€“Recall curve\n",
    "axes[1].plot(recall_rf,    prec_rf,    label=f\"RF (AP = {ap_rf:.3f})\")\n",
    "axes[1].plot(recall_fe,    prec_fe,    label=f\"XGB (FE) (AP = {ap_fe:.3f})\")\n",
    "axes[1].plot(recall_tuned, prec_tuned, label=f\"XGB Tuned (FE) (AP = {ap_tuned:.3f})\")\n",
    "# Baseline = prevalence\n",
    "axes[1].axhline((y_test == 1).mean(), ls='--', c='gray', lw=1, label=\"Baseline = prevalence\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Precisionâ€“Recall Curve\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# (3) Bar plot of thresholded metrics\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.23\n",
    "axes[2].bar(x - width,      df_metrics[\"RF (no FE)\"],         width, label=\"RF (no FE)\")\n",
    "axes[2].bar(x,              df_metrics[\"XGB (FE)\"],           width, label=\"XGB (FE)\")\n",
    "axes[2].bar(x + width,      df_metrics[\"XGB Tuned (FE)\"],     width, label=\"XGB Tuned (FE)\")\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(metrics)\n",
    "axes[2].set_ylabel(\"Score\")\n",
    "axes[2].set_ylim(0, 1.0)\n",
    "axes[2].set_title(\"Metric Comparison @ default threshold\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Model Comparison: RF (no FE) vs XGB (FE) vs XGB Tuned (FE)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fec3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_scores = [\n",
    "    accuracy_score(y_test, rf_pred),\n",
    "    precision_score(y_test, rf_pred, zero_division=0),\n",
    "    recall_score(y_test, rf_pred, zero_division=0),\n",
    "    f1_score(y_test, rf_pred, zero_division=0)\n",
    "]\n",
    "\n",
    "xgb_fe_scores = [\n",
    "    accuracy_score(y_test, y_pred_xgb),\n",
    "    precision_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    recall_score(y_test, y_pred_xgb, zero_division=0),\n",
    "    f1_score(y_test, y_pred_xgb, zero_division=0)\n",
    "]\n",
    "\n",
    "xgb_tuned_scores = [\n",
    "    accuracy_score(y_test, y_pred_xgb_tuned),\n",
    "    precision_score(y_test, y_pred_xgb_tuned, zero_division=0),\n",
    "    recall_score(y_test, y_pred_xgb_tuned, zero_division=0),\n",
    "    f1_score(y_test, y_pred_xgb_tuned, zero_division=0)\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    \"Metric\": metrics,\n",
    "    \"RF (no FE)\": rf_scores,\n",
    "    \"XGB (FE)\": xgb_fe_scores,\n",
    "    \"XGB Tuned (FE)\": xgb_tuned_scores\n",
    "})\n",
    "\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30267024",
   "metadata": {},
   "source": [
    "## Bar Plot Interpretation: Model Evaluation Metrics\n",
    "\n",
    "The bar plot compares four key evaluation metrics: Accuracy, Precision, Recall, and F1-score, across Random Forest, baseline XGBoost, and tuned XGBoost models on the test set. \n",
    "The main task is to identify defaulters (1) versus non-defaulters (0), in a dataset with a strong class imbalance (far fewer defaulters).\n",
    "\n",
    "1. Accuracy\n",
    "All three models achieve very high accuracy (around 95.8% to 96%). However, in this imbalanced dataset, where defaulters are much less common than non-defaulters, high accuracy is not a strong indicator of real-world performance. Most of this accuracy comes from correctly predicting the majority class (non-defaulters), while performance on the minority class (defaulters) is what truly matters.\n",
    "\n",
    "2. Precision, Recall, and F1-score\n",
    "Precision indicates how often a predicted defaulter is actually a defaulter. The values are fairly low (0.17 to 0.19), reflecting the difficulty in making correct positive predictions due to class imbalance.\n",
    "\n",
    "Recall measures how many actual defaulters the model successfully identifies. Here, the tuned XGBoost achieves the highest recall (0.455), followed closely by baseline XGBoost (0.447), and then Random Forest (0.366).\n",
    "\n",
    "F1-score is the harmonic mean of precision and recall, representing the overall balance between the two. The tuned XGBoost has the highest F1-score (0.271), indicating the best trade-off between precision and recall.\n",
    "\n",
    "\n",
    "Model Comparison:\n",
    "\n",
    "- Random Forest achieves the lowest precision, recall, and F1-score among the three, but still provides a meaningful baseline.\n",
    "\n",
    "- Baseline XGBoost improves upon Random Forest, especially in terms of recall and F1-score, suggesting itâ€™s better at finding defaulters and balancing false positives.\n",
    "\n",
    "- Tuned XGBoost slightly outperforms both Random Forest and baseline XGBoost on all key metrics, demonstrating that hyperparameter tuning has helped the model become even more effective at detecting defaulters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227497db",
   "metadata": {},
   "source": [
    "In the context of credit default prediction, it is crucial to find the right balance between identifying true defaulters and minimizing false alarms. The precision-recall curve is a valuable tool for visualizing this trade-off, as it shows how precision (the proportion of correctly identified defaulters among all predicted defaulters) and recall (the proportion of actual defaulters correctly identified by the model) change as we adjust the classification threshold. In practical terms, if our main concern is to avoid lending to customers who are likely to default, we may prioritize a higher recall, accepting a greater number of false positives to ensure we capture as many defaulters as possible. Conversely, if we want to avoid mistakenly classifying reliable customers as risky, we might set a higher threshold to achieve higher precision, even if it means some defaulters go undetected. By analyzing the precision-recall curve, we can select a threshold that aligns with our institutionâ€™s risk appetite and business objectives, ensuring our credit decision process is both responsible and aligned with our strategic goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392c03e",
   "metadata": {},
   "source": [
    "## Learning curve \n",
    "\n",
    "Plot that shows the relationship between the size of training dataset and the model's performance on both training and validation/test set. It tells us how the modelâ€™s performance improves (or stagnates) as it sees more data.\n",
    "\n",
    "1. XGBoost can be prone to overfitting if not tuned well, thus, we displayed the learning curve to see if the current hyperparameter settings is fitting the data well.\n",
    "- High training score but low validation score suggests model is overfitting.\n",
    "- Low training and validation scores suggest model is underfitting.\n",
    "\n",
    "\n",
    "2. See if adding more data would help:\n",
    "- If validation score is still improving with more data, more samples might help.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold\n",
    "\n",
    "# --- Unfitted model instances (match your training runs) ---\n",
    "rf_for_curve = RandomForestClassifier(random_state=42)  # baseline RF (no FE)\n",
    "\n",
    "xgb_base_for_curve = XGBClassifier(\n",
    "    n_estimators=200, max_depth=15, learning_rate=0.1,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "    random_state=42, eval_metric='logloss', n_jobs=-1\n",
    ")\n",
    "\n",
    "# clone tuned params but keep it UNFITTED for learning_curve\n",
    "tuned_xgb_for_curve = XGBClassifier(**best_xgb.get_params())\n",
    "\n",
    "# --- Stratified CV for imbalance ---\n",
    "skf5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- Learning curve helper (accepts X, y explicitly) ---\n",
    "def get_curve(estimator, X, y):\n",
    "    sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=estimator, X=X, y=y,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=skf5, scoring='roc_auc', n_jobs=-1,\n",
    "        shuffle=True, random_state=42\n",
    "    )\n",
    "    return sizes, val_scores.mean(axis=1), val_scores.std(axis=1)\n",
    "\n",
    "# RF on RAW features; XGBs on FE/selected features\n",
    "sizes_rf,    val_rf,    std_rf    = get_curve(rf_for_curve,        X_train,           y_train)\n",
    "sizes_base,  val_base,  std_base  = get_curve(xgb_base_for_curve,  X_train_selected,  y_train)\n",
    "sizes_tuned, val_tuned, std_tuned = get_curve(tuned_xgb_for_curve, X_train_selected,  y_train)\n",
    "\n",
    "# --- Plot (unchanged, just using val_* / std_*) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.fill_between(sizes_rf,    val_rf - std_rf,    val_rf + std_rf,    alpha=0.1)\n",
    "plt.fill_between(sizes_base,  val_base - std_base,  val_base + std_base,  alpha=0.1)\n",
    "plt.fill_between(sizes_tuned, val_tuned - std_tuned, val_tuned + std_tuned, alpha=0.1)\n",
    "\n",
    "plt.plot(sizes_rf,   val_rf,   'o-', label=\"RF (no FE)\",      zorder=3)\n",
    "plt.plot(sizes_base, val_base, 's--', label=\"XGB (FE)\",       zorder=2)\n",
    "plt.plot(sizes_tuned,val_tuned,'d-.', label=\"XGB Tuned (FE)\", zorder=4)\n",
    "\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Validation ROC AUC (Stratified 5-fold)')\n",
    "plt.title('Learning Curves: RF (no FE) vs XGB (FE) vs XGB Tuned (FE)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82d9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a819446",
   "metadata": {},
   "source": [
    "X-axis: Number of training samples used (from small to large subsets of your data).\n",
    "\n",
    "Y-axis: Validation ROC AUC (how well the model ranks positive vs negative cases on held-out folds).\n",
    "\n",
    "Curves: Each line shows how each modelâ€™s performance improves as more data is used for training.\n",
    "\n",
    "Shaded area: Â±1 standard deviation across CV folds, showing variability.\n",
    "\n",
    "\n",
    "Interpretation\n",
    "1. Performance Improves with More Data\n",
    "All models show increasing validation ROC AUC as the number of training samples increases. This is expectedâ€”more data generally leads to better generalization.\n",
    "\n",
    "2. Random Forest Outperforms XGBoost at All Sample Sizes\n",
    "\n",
    "3. Baseline and Tuned XGBoost are Similar, but Tuned Overtakes at Larger Size\n",
    "As the sample size increases, the tuned XGBoost pulls ahead slightly, showing the benefit of hyperparameter tuning\n",
    "\n",
    "4. Variance Decreases with More Data\n",
    "The shaded regions (standard deviation) are widest at the smallest sample sizes and narrow as more data is used. This indicates more stable, reliable performance estimates at higher sample sizes.\n",
    "\n",
    "5. All Models Have Not Plateaued\n",
    "The ROC AUC still increases as you approach the largest sample size. This suggests that even more data could yield further gains, none of the models have fully saturated their learning capacity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs3244",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
